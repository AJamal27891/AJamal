{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "License Plate Detection.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AJamal27891/AJamal/blob/master/License_Plate_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azmHl4WG_qeq",
        "colab_type": "text"
      },
      "source": [
        "[link text](https://)\n",
        "\n",
        "1.   Ahmed Gamal: 319730618\n",
        "2.   Zara Akhter: 319712229\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHOmQaJfDJRC",
        "colab_type": "text"
      },
      "source": [
        "Main Steps\n",
        "\n",
        "\n",
        "\n",
        "*  Detect car in input image using Yolo. In this method we had no struggle since we imported Darknet API. However, in order to train on a specific class, it gave a lot of errors.\n",
        "*   Detect the plate within the carâ€™s contour area.\n",
        "*   Crop the license plate.\n",
        "*   We applied two methods:\n",
        "\n",
        "1.   Train a Neural Network (NN) on eMinst dataset.\n",
        "2.   Train a Convolutional Neural Network (CNN) on the plate regions\n",
        "\n",
        "*   Identify each character and calculate the loss\n",
        "*   An additional method to calculate based on the 7-character combinations.\n",
        "\n",
        "Datasets \n",
        "\n",
        "*   EMNIST Balanced Dataset:\n",
        "1.  131,600 characters.\n",
        "2.  47 balanced classes. \n",
        "*   License plate Dataset\n",
        "\n",
        "We trained our CNN eMNIST dataset since it includes alphabets along with the numerical digits. Our CNN consisted of:\n",
        "\n",
        "*    2 convolution layers.\n",
        "*    2 maxpooling layers.\n",
        "*    2 linear layers.\n",
        "*    Optimizer Adam to find the best gradient.\n",
        "\n",
        "We also added dropout layer as regularization since our model was overfitting with 99% accuracy on the training set. The accuracy reduced to 98% with the dropout regularization.\n",
        "\n",
        "Additionally, we applied GridSearch to extract the best parameters for our model which were:\n",
        "\n",
        "*    Learning Rate = 0.006\n",
        "*    Max epochs = 10\n",
        "\n",
        "This resulted in an accuracy of 92\n",
        "\n",
        "Final Archetechre 2 layers of convolution 1 layer of Maxpooling three times then big flattend dense layer then 7 paralel dense layers every layer predict charchter. \n",
        "Model: \"model_2\"\n",
        "__________________________________________________________________________________________________\n",
        "Layer (type)                    Output Shape         Param #     Connected to                     \n",
        "==================================================================================================\n",
        "input_2 (InputLayer)            (None, 218, 1025, 3) 0                                            \n",
        "__________________________________________________________________________________________________\n",
        "conv2d_7 (Conv2D)               (None, 216, 1023, 8) 224         input_2[0][0]                    \n",
        "__________________________________________________________________________________________________\n",
        "conv2d_8 (Conv2D)               (None, 214, 1021, 8) 584         conv2d_7[0][0]                   \n",
        "__________________________________________________________________________________________________\n",
        "max_pooling2d_4 (MaxPooling2D)  (None, 107, 510, 8)  0           conv2d_8[0][0]                   \n",
        "__________________________________________________________________________________________________\n",
        "conv2d_9 (Conv2D)               (None, 105, 508, 16) 1168        max_pooling2d_4[0][0]            \n",
        "__________________________________________________________________________________________________\n",
        "conv2d_10 (Conv2D)              (None, 103, 506, 16) 2320        conv2d_9[0][0]                   \n",
        "__________________________________________________________________________________________________\n",
        "max_pooling2d_5 (MaxPooling2D)  (None, 51, 253, 16)  0           conv2d_10[0][0]                  \n",
        "__________________________________________________________________________________________________\n",
        "conv2d_11 (Conv2D)              (None, 49, 251, 16)  2320        max_pooling2d_5[0][0]            \n",
        "__________________________________________________________________________________________________\n",
        "conv2d_12 (Conv2D)              (None, 47, 249, 16)  2320        conv2d_11[0][0]                  \n",
        "__________________________________________________________________________________________________\n",
        "max_pooling2d_6 (MaxPooling2D)  (None, 23, 124, 16)  0           conv2d_12[0][0]                  \n",
        "__________________________________________________________________________________________________\n",
        "flatten_2 (Flatten)             (None, 45632)        0           max_pooling2d_6[0][0]            \n",
        "__________________________________________________________________________________________________\n",
        "dense_9 (Dense)                 (None, 64)           2920512     flatten_2[0][0]                  \n",
        "__________________________________________________________________________________________________\n",
        "dense_10 (Dense)                (None, 33)           2145        dense_9[0][0]                    \n",
        "__________________________________________________________________________________________________\n",
        "dense_11 (Dense)                (None, 33)           2145        dense_9[0][0]                    \n",
        "__________________________________________________________________________________________________\n",
        "dense_12 (Dense)                (None, 33)           2145        dense_9[0][0]                    \n",
        "__________________________________________________________________________________________________\n",
        "dense_13 (Dense)                (None, 33)           2145        dense_9[0][0]                    \n",
        "__________________________________________________________________________________________________\n",
        "dense_14 (Dense)                (None, 33)           2145        dense_9[0][0]                    \n",
        "__________________________________________________________________________________________________\n",
        "dense_15 (Dense)                (None, 33)           2145        dense_9[0][0]                    \n",
        "__________________________________________________________________________________________________\n",
        "dense_16 (Dense)                (None, 33)           2145        dense_9[0][0]                    \n",
        "==================================================================================\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s5XupvYoLM56",
        "colab": {}
      },
      "source": [
        "#@title Import libraries { display-mode: \"form\" }\n",
        "from torchvision.datasets import EMNIST\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "import torch.nn as nn\n",
        "import torch.functional as F\n",
        "import torch.optim as optim\n",
        "import gzip\n",
        "import os \n",
        "import struct\n",
        "import pandas as pd\n",
        "from torchvision import transforms\n",
        "import random\n",
        "from imutils import perspective\n",
        "from PIL import Image, ImageFilter\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "from PIL import ImageOps\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import ToTensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "form",
        "id": "owCj4JimLM57",
        "colab": {}
      },
      "source": [
        "#@title GPU Code\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "form",
        "id": "NBXBOozKLM59",
        "colab": {}
      },
      "source": [
        "#@title TPU code\n",
        "import os\n",
        "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "form",
        "id": "MbGxr5TQLM5_",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import requests\n",
        "import threading\n",
        "\n",
        "_VersionConfig = collections.namedtuple('_VersionConfig', 'wheels,server')\n",
        "VERSION = \"torch_xla==nightly\"  #@param [\"xrt==1.15.0\", \"torch_xla==nightly\"]\n",
        "CONFIG = {\n",
        "    'xrt==1.15.0': _VersionConfig('1.15', '1.15.0'),\n",
        "    'torch_xla==nightly': _VersionConfig('nightly', 'XRT-dev{}'.format(\n",
        "        (datetime.today() - timedelta(1)).strftime('%Y%m%d'))),\n",
        "}[VERSION]\n",
        "DIST_BUCKET = 'gs://tpu-pytorch/wheels'\n",
        "TORCH_WHEEL = 'torch-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
        "TORCH_XLA_WHEEL = 'torch_xla-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
        "TORCHVISION_WHEEL = 'torchvision-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n",
        "\n",
        "# Update TPU XRT version\n",
        "def update_server_xrt():\n",
        "  print('Updating server-side XRT to {} ...'.format(CONFIG.server))\n",
        "  url = 'http://{TPU_ADDRESS}:8475/requestversion/{XRT_VERSION}'.format(\n",
        "      TPU_ADDRESS=os.environ['COLAB_TPU_ADDR'].split(':')[0],\n",
        "      XRT_VERSION=CONFIG.server,\n",
        "  )\n",
        "  print('Done updating server-side XRT: {}'.format(requests.post(url)))\n",
        "\n",
        "update = threading.Thread(target=update_server_xrt)\n",
        "update.start()\n",
        "\n",
        "# Install Colab TPU compat PyTorch/TPU wheels and dependencies\n",
        "!pip uninstall -y torch torchvision\n",
        "!gsutil cp \"$DIST_BUCKET/$TORCH_WHEEL\" .\n",
        "!gsutil cp \"$DIST_BUCKET/$TORCH_XLA_WHEEL\" .\n",
        "!gsutil cp \"$DIST_BUCKET/$TORCHVISION_WHEEL\" .\n",
        "!pip install \"$TORCH_WHEEL\"\n",
        "!pip install \"$TORCH_XLA_WHEEL\"\n",
        "!pip install \"$TORCHVISION_WHEEL\"\n",
        "!sudo apt-get install libomp5\n",
        "update.join()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q7lhOSCALM6A",
        "colab": {}
      },
      "source": [
        "#@title Connect to Google { display-mode: \"form\" }\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/Colab Notebooks'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "form",
        "id": "yLa63RhBLM6C",
        "colab": {}
      },
      "source": [
        "#@title Class for transforming the data into torch dataset\n",
        "from torch.utils.data import  Dataset, DataLoader\n",
        "from torch.autograd import  Variable\n",
        "class Train(Dataset):\n",
        "  def __init__(self,transform=None):\n",
        "    xy=torch.tensor(pd.read_csv('gdrive/My Drive/Colab Notebooks/emnist-balanced-train.csv').values)\n",
        "    self.len = xy.shape[0]\n",
        "    self.train_x = xy[:,1:].view(112799,1,28,28)\n",
        "    self.train_y = xy[:, 0:1].view(1,112799)\n",
        "    self.transform = transform\n",
        "  def __getitem__ (self,index):\n",
        "    if self.transform:\n",
        "      train_x = self.transform \n",
        "    return self.train_x[index][-1],self.train_y[index][-1]\n",
        "  def __len__(self):\n",
        "    return self.len\n",
        "\n",
        "class Test(Dataset):\n",
        "  def __init__(self):\n",
        "    xy=np.loadtxt('gdrive/My Drive/Colab Notebooks/emnist-balanced-test.csv',delimiter=',',dtype=np.float32)\n",
        "    self.len = xy.shape[0]\n",
        "    self.test_x = torch.from_numpy(xy[:,-1:])\n",
        "    self.test_y = torch.from_numpy(xy[:,[-1]])\n",
        "  def __getitem__ (self,index):\n",
        "    return self.test_x[index+1],self.test_y[index+1]\n",
        "  def __len__(self):\n",
        "    return self.len\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "form",
        "id": "OzmNtTBPLM6D",
        "colab": {}
      },
      "source": [
        "#@title Custom Emnist\n",
        "from torchvision.datasets import MNIST\n",
        "from __future__ import print_function\n",
        "import os\n",
        "import shutil\n",
        "import tempfile\n",
        "import torch\n",
        "from torchvision.datasets.folder import ImageFolder\n",
        "from torchvision.datasets.utils import check_integrity, download_and_extract_archive, extract_archive, \\\n",
        "    verify_str_arg\n",
        "\n",
        "ARCHIVE_DICT = {\n",
        "    'train': {\n",
        "        'url': 'http://www.image-net.org/challenges/LSVRC/2012/nnoupb/ILSVRC2012_img_train.tar',\n",
        "        'md5': '1d675b47d978889d74fa0da5fadfb00e',\n",
        "    },\n",
        "    'val': {\n",
        "        'url': 'http://www.image-net.org/challenges/LSVRC/2012/nnoupb/ILSVRC2012_img_val.tar',\n",
        "        'md5': '29b22e2961454d5413ddabcf34fc5622',\n",
        "    },\n",
        "    'devkit': {\n",
        "        'url': 'http://www.image-net.org/challenges/LSVRC/2012/nnoupb/ILSVRC2012_devkit_t12.tar.gz',\n",
        "        'md5': 'fa75699e90414af021442c21a62c3abf',\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "class EMNIST(MNIST):\n",
        "    \"\"\"`EMNIST <https://www.westernsydney.edu.au/bens/home/reproducible_research/emnist>`_ Dataset.\n",
        "\n",
        "    Args:\n",
        "        root (string): Root directory of dataset where ``EMNIST/processed/training.pt``\n",
        "            and  ``EMNIST/processed/test.pt`` exist.\n",
        "        split (string): The dataset has 6 different splits: ``byclass``, ``bymerge``,\n",
        "            ``balanced``, ``letters``, ``digits`` and ``mnist``. This argument specifies\n",
        "            which one to use.\n",
        "        train (bool, optional): If True, creates dataset from ``training.pt``,\n",
        "            otherwise from ``test.pt``.\n",
        "        download (bool, optional): If true, downloads the dataset from the internet and\n",
        "            puts it in root directory. If dataset is already downloaded, it is not\n",
        "            downloaded again.\n",
        "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
        "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
        "        target_transform (callable, optional): A function/transform that takes in the\n",
        "            target and transforms it.\n",
        "    \"\"\"\n",
        "    # Updated URL from https://www.nist.gov/node/1298471/emnist-dataset since the\n",
        "    # _official_ download link\n",
        "    # https://cloudstor.aarnet.edu.au/plus/s/ZNmuFiuQTqZlu9W/download\n",
        "    # is (currently) unavailable\n",
        "    url = 'https://cloudstor.aarnet.edu.au/plus/s/ZNmuFiuQTqZlu9W/download'\n",
        "    md5 = \"58c8d27c78d21e728a6bc7b3cc06412e\"\n",
        "    splits = ('byclass', 'bymerge', 'balanced', 'letters', 'digits', 'mnist')\n",
        "\n",
        "    def __init__(self, root, split, **kwargs):\n",
        "        self.split = verify_str_arg(split, \"split\", self.splits)\n",
        "        self.training_file = self._training_file(split)\n",
        "        self.test_file = self._test_file(split)\n",
        "        super(EMNIST, self).__init__(root, **kwargs)\n",
        "\n",
        "    @staticmethod\n",
        "    def _training_file(split):\n",
        "        return 'training_{}.pt'.format(split)\n",
        "\n",
        "    @staticmethod\n",
        "    def _test_file(split):\n",
        "        return 'test_{}.pt'.format(split)\n",
        "\n",
        "    def download(self):\n",
        "        \"\"\"Download the EMNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
        "        import shutil\n",
        "\n",
        "        if self._check_exists():\n",
        "            return\n",
        "\n",
        "        makedir_exist_ok(self.raw_folder)\n",
        "        makedir_exist_ok(self.processed_folder)\n",
        "\n",
        "        # download files\n",
        "        print('Downloading and extracting zip archive')\n",
        "        download_and_extract_archive(self.url, download_root=self.raw_folder, filename=\"emnist.zip\",\n",
        "                                     remove_finished=True, md5=self.md5)\n",
        "        gzip_folder = os.path.join(self.raw_folder, 'gzip')\n",
        "        for gzip_file in os.listdir(gzip_folder):\n",
        "            if gzip_file.endswith('.gz'):\n",
        "                extract_archive(os.path.join(gzip_folder, gzip_file), gzip_folder)\n",
        "\n",
        "        # process and save as torch files\n",
        "        for split in self.splits:\n",
        "            print('Processing ' + split)\n",
        "            training_set = (\n",
        "                read_image_file(os.path.join(gzip_folder, 'emnist-{}-train-images-idx3-ubyte'.format(split))),\n",
        "                read_label_file(os.path.join(gzip_folder, 'emnist-{}-train-labels-idx1-ubyte'.format(split)))\n",
        "            )\n",
        "            test_set = (\n",
        "                read_image_file(os.path.join(gzip_folder, 'emnist-{}-test-images-idx3-ubyte'.format(split))),\n",
        "                read_label_file(os.path.join(gzip_folder, 'emnist-{}-test-labels-idx1-ubyte'.format(split)))\n",
        "            )\n",
        "            with open(os.path.join(self.processed_folder, self._training_file(split)), 'wb') as f:\n",
        "                torch.save(training_set, f)\n",
        "            with open(os.path.join(self.processed_folder, self._test_file(split)), 'wb') as f:\n",
        "                torch.save(test_set, f)\n",
        "        shutil.rmtree(gzip_folder)\n",
        "\n",
        "        print('Done!')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "form",
        "id": "AIgVlIlsLM6F",
        "colab": {}
      },
      "source": [
        "#@title extract ubyte method\n",
        "from torchvision.datasets import DatasetFolder  \n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets.vision import VisionDataset\n",
        "from torchvision.datasets.folder import default_loader, make_dataset\n",
        "\n",
        "def find_classes(dir):\n",
        "    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
        "    classes.sort()\n",
        "    class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
        "    return classes, class_to_idx\n",
        "\n",
        "\n",
        "class imgesfolder(Dataset):\n",
        "  def __init__(self,root,transform=None,traget_transform=None,loader=default_loader):\n",
        "    classes, class_to_idx = find_classes(root)\n",
        "    imgs = make_dataset(root,class_to_idx,extensions='txt')\n",
        "    if len(imgs) == 0:\n",
        "      raise(RuntimeError('no files'))\n",
        "    self.root = root\n",
        "    self.imgs = imags\n",
        "    self.class_to_idx = class_to_idx\n",
        "    self.classes = classes \n",
        "    self.transform=transform\n",
        "    self.traget_transform=target_transform\n",
        "    self.loader=loader\n",
        "  def __getitem__(self,index):\n",
        "    path, target = self.imgs[index]\n",
        "    img = self.loader(path)\n",
        "    if self.transform is None:\n",
        "      img = self.transform(img)\n",
        "    if self.target_transform is None:\n",
        "      target =self.target_transform(target)\n",
        "    return img, target\n",
        "  def __len__(self):\n",
        "    return len(self.imgs)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "form",
        "id": "tIxKbFp0LM6G",
        "colab": {}
      },
      "source": [
        "#@title Custom Class for EMNIST\n",
        "def makedir_exist_ok(dirpath):\n",
        "    \"\"\"\n",
        "    Python2 support for os.makedirs(.., exist_ok=True)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        os.makedirs(dirpath)\n",
        "    except OSError as e:\n",
        "        if e.errno == errno.EEXIST:\n",
        "            pass\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "from __future__ import print_function\n",
        "import torch.utils.data as data\n",
        "from PIL import Image\n",
        "import os\n",
        "import os.path\n",
        "import gzip\n",
        "import numpy as np\n",
        "import torch\n",
        "import codecs\n",
        "import errno\n",
        "import hashlib\n",
        "from tqdm import tqdm\n",
        "\n",
        "def read_image_file(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        data = f.read()\n",
        "        assert get_int(data[:4]) == 2051\n",
        "        length = get_int(data[4:8])\n",
        "        num_rows = get_int(data[8:12])\n",
        "        num_cols = get_int(data[12:16])\n",
        "        parsed = np.frombuffer(data, dtype=np.uint8, offset=16)\n",
        "        return torch.from_numpy(parsed).view(length, num_rows, num_cols)                \n",
        "\n",
        "#%%\n",
        "def read_label_file(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        data = f.read()\n",
        "        assert get_int(data[:4]) == 2049\n",
        "        length = get_int(data[4:8])\n",
        "        parsed = np.frombuffer(data, dtype=np.uint8, offset=8)\n",
        "        return torch.from_numpy(parsed).view(length).long()    \n",
        "\n",
        "def get_int(b):\n",
        "    return int(codecs.encode(b, 'hex'), 16)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "form",
        "id": "_QfziXMELM6I",
        "colab": {}
      },
      "source": [
        "#@title DataLoader\n",
        "\n",
        "\n",
        "train_set = EMNIST(root='EMNIST/processed/training.pt',split='balanced',train=True,download=True,transform = transforms.Compose([transforms.ToTensor()]))\n",
        "test_set = EMNIST(root='EMNIST/processed/training.pt',split='balanced',train=False,download=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "form",
        "id": "nsrq--QqLM6K",
        "colab": {}
      },
      "source": [
        "#@title Load the train and the test to the DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from PIL import ImageOps\n",
        "from torchvision import datasets, transforms\n",
        "transformation = transforms.Compose([transforms.RandomHorizontalFlip(),transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n",
        "train_data = Train(transform=transformation)\n",
        "test_data = Test()\n",
        "loader = DataLoader(train_data, batch_size=10)\n",
        "test_loader = DataLoader(test_data,batch_size=10000,shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sV7UaDIzLM6M",
        "colab": {}
      },
      "source": [
        "def get_all_preds(model,loader):\n",
        "  all_preds= torch.tensor([])\n",
        "  for batch in loader:\n",
        "    images, labels = batch\n",
        "    preds = model(images)\n",
        "    all_preds = torch.cat((all_preds,preds),dim=0)\n",
        "\n",
        "  return all_preds\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "both",
        "id": "jEunrFoNLM6N",
        "colab": {}
      },
      "source": [
        "#@title DataLoader\n",
        "batch = 10000\n",
        "train_loader = DataLoader(train_set,batch_size=batch,shuffle=True)\n",
        "test_loader = DataLoader(test_set,batch_size=batch,shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XO493ebeLM6P",
        "colab": {}
      },
      "source": [
        "#@title Building CNN arch\n",
        "torch.set_grad_enabled(True)\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=1 , out_channels=6 , kernel_size=5 )\n",
        "    self.conv2 = nn.Conv2d(in_channels=6 , out_channels=12 , kernel_size=5 )\n",
        "\n",
        "\n",
        "    self.fc1 =nn.Linear(in_features=12*4*4 , out_features= 120)\n",
        "    self.fc2 = nn.Linear(in_features=120 , out_features=60 )\n",
        "    self.out = nn.Linear(in_features=60 , out_features=47) \n",
        "    \n",
        "  def forward(self,t):\n",
        "    #input layer(1)\n",
        "    \n",
        "    t=t\n",
        "    #hidden layer(2)\n",
        "    t =self.conv1(t)\n",
        "    t= F.relu(t)\n",
        "    t= F.max_pool2d(t,kernel_size=2,stride=2)\n",
        "    #hidden layer(3)\n",
        "    t =self.conv2(t)\n",
        "    t= F.relu(t)\n",
        "    t= F.max_pool2d(t,kernel_size=2,stride=2)\n",
        "    #hidden linear layer (4)\n",
        "    t = t.reshape(-1,12*4*4)\n",
        "    t= self.fc1(t)\n",
        "    t= F.relu(t)\n",
        "    #hidden linear layer (5)\n",
        "    t= self.fc2(t)\n",
        "    t= F.relu(t)\n",
        "  \n",
        "    #outputlayer\n",
        "    t =self.out(t)\n",
        "    t = F.softmax(t,dim=1)\n",
        "    return t\n",
        "net = Net()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iypw18fXLM6Q",
        "colab": {}
      },
      "source": [
        "sample = next(iter(train_set))\n",
        "image,label = sample\n",
        "image.shape\n",
        "image= image.unsqueeze(0)\n",
        "image.shape\n",
        "pred = net(image)\n",
        "print(pred)\n",
        "print(label)\n",
        "plt.imshow(image.view(28,28),cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1Xh4xbZtLM6S",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(net.parameters(),lr=0.01)\n",
        "\n",
        "def get_num_correct(pred,labels):\n",
        "        return pred.argmax(dim=1).eq(labels).sum().item()\n",
        "\n",
        "\n",
        "accuracy_tabel = []\n",
        "\n",
        "for epoch in range(50):\n",
        "  total_loss = 0\n",
        "  total_correct = 0\n",
        "\n",
        "  for batch in train_loader:\n",
        "    images,labels = batch \n",
        "\n",
        "    pred = net(images)\n",
        "\n",
        "    loss = F.cross_entropy(pred,labels)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_loss += loss.item()\n",
        "    total_correct += get_num_correct(pred,labels)\n",
        "    accuracy = 1-(total_loss/total_correct)\n",
        "    accurcy_table = accuracy_tabel.append(accuracy)\n",
        "  \n",
        "\n",
        "  print('epoch',epoch,'total correct',total_correct, 'total loss', total_loss, 'accuracy ',accuracy )\n",
        "\n",
        "accuracy_dict = { 'accuracy':[accuracy_tabel] }\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GEUxa1TgLM6T",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from skorch import NeuralNetClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "new_net = NeuralNetClassifier(\n",
        "    net,\n",
        "    max_epochs=10,\n",
        "    lr=0.1,\n",
        "    # Shuffle training data on each epoch\n",
        "    iterator_train__shuffle=True,\n",
        ")\n",
        "\n",
        "new_net.fit(images, labels)\n",
        "y_proba = new_net.predict_proba(images)\n",
        "\n",
        "params = {\n",
        "    'lr': [0.01, 0.02,0.001],\n",
        "    'max_epochs': [10, 20],\n",
        "}\n",
        "gs = GridSearchCV(new_net, params, refit=False, cv=30, scoring='accuracy')\n",
        "\n",
        "gs.fit(np.array(images), np.array(labels))\n",
        "print(gs.best_score_, gs.best_params_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "haZVbAuzLM6W",
        "colab": {}
      },
      "source": [
        "print(torchvision.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ip2LHRyeLM6X",
        "colab": {}
      },
      "source": [
        "prediction_loader = torch.utils.data.DataLoader(train_set,batch_size=10000)\n",
        "train_preds = get_all_preds(net,prediction_loader) \n",
        "train_preds.shape\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "31uvA1P6LM6Z",
        "colab": {}
      },
      "source": [
        "print(train_preds.requires_grad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DOAr8ee5LM6b",
        "colab": {}
      },
      "source": [
        "with torch.no_grad():\n",
        "  prediction_loader = torch.utils.data.DataLoader(train_set,batch_size=10000)\n",
        "  train_preds = get_all_preds(net,prediction_loader) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OSbrKOZZLM6d",
        "colab": {}
      },
      "source": [
        "train_preds.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f96I7e1gLM6e",
        "colab": {}
      },
      "source": [
        "pred_correct = get_num_correct(train_preds,train_set.targets)\n",
        "pred_correct"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kTGDNXavLM6f",
        "colab": {}
      },
      "source": [
        "stacked = torch.stack((train_set.targets,train_preds.argmax(dim=1)),dim=1)\n",
        "stacked"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LflN9i13LM6h",
        "colab": {}
      },
      "source": [
        "stacked[0].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lALFxCX3LM6i",
        "colab": {}
      },
      "source": [
        "cmt=torch.zeros(47,47,dtype= torch.int32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GNRXuj2uLM6k",
        "colab": {}
      },
      "source": [
        "for p in stacked:\n",
        "  j,k = p.tolist()\n",
        "  cmt[j,k]= cmt[j,k]+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c4xpPl_aLM6m",
        "colab": {}
      },
      "source": [
        "cmt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uclMTS1Ed-qu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-ushlQ6d7yg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd drive//My\\ Drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x00S2l9TcGQa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "import cv2\n",
        "import os\n",
        "import random\n",
        "\n",
        "\n",
        "num_epochs = 300\n",
        "\n",
        "\n",
        "INITIAL_LEARNING_RATE = 1e-3\n",
        "DECAY_STEPS = 2000\n",
        "LEARNING_RATE_DECAY_FACTOR = 0.9  # The learning rate decay factor\n",
        "MOMENTUM = 0.9\n",
        "\n",
        "REPORT_STEPS = 5000\n",
        "\n",
        "\n",
        "BATCH_SIZE = 50\n",
        "TRAIN_SIZE = 7368\n",
        "BATCHES = TRAIN_SIZE//BATCH_SIZE\n",
        "test_num = 3\n",
        "\n",
        "ti = 'train'        \n",
        "vi = 'valid'        \n",
        "img_size = [94, 24]\n",
        "tl = None\n",
        "vl = None\n",
        "num_channels = 3\n",
        "label_len = 7\n",
        "\n",
        "CHARS = [\n",
        "         '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "         'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K',\n",
        "         'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V',\n",
        "         'W', 'X', 'Y', 'Z','-','O','I'\n",
        "         ]\n",
        "\n",
        "CHARS_DICT = {char:i for i, char in enumerate(CHARS)}\n",
        "\n",
        "NUM_CHARS = len(CHARS)\n",
        "\n",
        "\n",
        "def encode_label(s):\n",
        "    label = np.zeros([len(s)])\n",
        "    for i, c in enumerate(s):\n",
        "        label[i] = CHARS_DICT[c]\n",
        "    return label\n",
        "\n",
        "class TextImageGenerator:\n",
        "    def __init__(self, img_dir, label_file, batch_size, img_size, num_channels=3, label_len=7):\n",
        "        self._img_dir = img_dir\n",
        "        self._label_file = label_file\n",
        "        self._batch_size = batch_size\n",
        "        self._num_channels = num_channels\n",
        "        self._label_len = label_len\n",
        "        self._img_w, self._img_h = img_size\n",
        "\n",
        "        self._num_examples = 0\n",
        "        self._next_index = 0\n",
        "        self._num_epoches = 0\n",
        "        self.filenames = []\n",
        "        self.labels = []\n",
        "\n",
        "        self.init()\n",
        "\n",
        "    def init(self):\n",
        "        self.labels = []\n",
        "        fs = os.listdir(self._img_dir)\n",
        "        for filename in fs:\n",
        "                self.filenames.append(filename)\n",
        "        for filename in self.filenames:\n",
        "            label = filename[:7]\n",
        "            \n",
        "            label = encode_label(label)\n",
        "            self.labels.append(label)\n",
        "            self._num_examples += 1\n",
        "        self.labels = np.float32(self.labels)\n",
        "\n",
        "    def next_batch(self):\n",
        "        # Shuffle the data\n",
        "        if self._next_index == 0:\n",
        "            perm = np.arange(self._num_examples)\n",
        "            np.random.shuffle(perm)\n",
        "            self._filenames = [self.filenames[i] for i in perm]\n",
        "            self._labels = self.labels[perm]\n",
        "\n",
        "        batch_size = self._batch_size\n",
        "        start = self._next_index\n",
        "        end = self._next_index + batch_size\n",
        "        if end > self._num_examples:\n",
        "            self._next_index = 0\n",
        "            start = self._next_index\n",
        "            end = self._next_index + batch_size\n",
        "            self._num_epoches += 1\n",
        "        else:\n",
        "            self._next_index = end\n",
        "        images = np.zeros([batch_size, self._img_h, self._img_w, self._num_channels])\n",
        "        # labels = np.zeros([batch_size, self._label_len])\n",
        "\n",
        "        for j, i in enumerate(range(start, end)):\n",
        "            fname = self._filenames[i]\n",
        "            img = cv2.imread(os.path.join(self._img_dir, fname))\n",
        "            img = cv2.resize(img, (self._img_w, self._img_h), interpolation=cv2.INTER_CUBIC)\n",
        "            images[j, ...] = img\n",
        "        images = np.transpose(images, axes=[0, 2, 1, 3])\n",
        "        labels = self._labels[start:end, ...]\n",
        "        targets = [np.asarray(i) for i in labels]\n",
        "        sparse_labels = sparse_tuple_from(targets)\n",
        "        # input_length = np.zeros([batch_size, 1])\n",
        "\n",
        "        seq_len = np.ones(self._batch_size) * 24\n",
        "        return images, sparse_labels, seq_len\n",
        "\n",
        "def sparse_tuple_from(sequences, dtype=np.int32):\n",
        "    \"\"\"\n",
        "    Create a sparse representention of x.\n",
        "    Args:\n",
        "        sequences: a list of lists of type dtype where each element is a sequence\n",
        "    Returns:\n",
        "        A tuple with (indices, values, shape)\n",
        "    \"\"\"\n",
        "    indices = []\n",
        "    values = []\n",
        "\n",
        "    for n, seq in enumerate(sequences):\n",
        "        indices.extend(zip([n] * len(seq), range(len(seq))))\n",
        "        values.extend(seq)\n",
        "\n",
        "    indices = np.asarray(indices, dtype=np.int64)\n",
        "    values = np.asarray(values, dtype=dtype)\n",
        "    shape = np.asarray([len(sequences), np.asarray(indices).max(0)[1] + 1], dtype=np.int64)\n",
        "\n",
        "    return indices, values, shape\n",
        "\n",
        "\n",
        "def decode_sparse_tensor(sparse_tensor):\n",
        "    decoded_indexes = list()\n",
        "    current_i = 0\n",
        "    current_seq = []\n",
        "    for offset, i_and_index in enumerate(sparse_tensor[0]):\n",
        "        i = i_and_index[0]\n",
        "        if i != current_i:\n",
        "            decoded_indexes.append(current_seq)\n",
        "            current_i = i\n",
        "            current_seq = list()\n",
        "        current_seq.append(offset)\n",
        "    decoded_indexes.append(current_seq)\n",
        "    result = []\n",
        "    for index in decoded_indexes:\n",
        "        result.append(decode_a_seq(index, sparse_tensor))\n",
        "    return result\n",
        "\n",
        "\n",
        "def decode_a_seq(indexes, spars_tensor):\n",
        "    decoded = []\n",
        "    for m in indexes:\n",
        "        str = CHARS[spars_tensor[1][m]]\n",
        "        decoded.append(str)\n",
        "    return decoded\n",
        "\n",
        "def small_basic_block(x,im,om):\n",
        "    x = conv(x,im,int(om/4),ksize=[1,1])\n",
        "    x = tf.nn.relu(x)\n",
        "    x = conv(x,int(om/4),int(om/4),ksize=[3,1],pad='SAME')\n",
        "    x = tf.nn.relu(x)\n",
        "    x = conv(x,int(om/4),int(om/4),ksize=[1,3],pad='SAME')\n",
        "    x = tf.nn.relu(x)\n",
        "    x = conv(x,int(om/4),om,ksize=[1,1])\n",
        "    return x\n",
        "\n",
        "def conv(x,im,om,ksize,stride=[1,1,1,1],pad = 'SAME'):\n",
        "    conv_weights = tf.Variable(\n",
        "        tf.truncated_normal([ksize[0], ksize[1], im, om],  # 5x5 filter, depth 32.\n",
        "                            stddev=0.1,\n",
        "                            seed=None, dtype=tf.float32))\n",
        "    conv_biases = tf.Variable(tf.zeros([om], dtype=tf.float32))\n",
        "    out = tf.nn.conv2d(x,\n",
        "                        conv_weights,\n",
        "                        strides=stride,\n",
        "                        padding=pad)\n",
        "    relu = tf.nn.bias_add(out, conv_biases)\n",
        "    return relu\n",
        "\n",
        "def get_train_model(num_channels, label_len, b, img_size):\n",
        "    inputs = tf.placeholder(\n",
        "        tf.float32,\n",
        "        shape=(b, img_size[0], img_size[1], num_channels))\n",
        "\n",
        "  #ctc_loss\n",
        "    targets = tf.sparse_placeholder(tf.int32)\n",
        "\n",
        "   #[batch_size,]\n",
        "    seq_len = tf.placeholder(tf.int32, [None])\n",
        "    x = inputs\n",
        "\n",
        "    x = conv(x,num_channels,64,ksize=[3,3])\n",
        "    x = tf.layers.batch_normalization(x)\n",
        "    x = tf.nn.relu(x)\n",
        "    x = tf.nn.max_pool(x,\n",
        "                          ksize=[1, 3, 3, 1],\n",
        "                          strides=[1, 1, 1, 1],\n",
        "                          padding='SAME')\n",
        "    x = small_basic_block(x,64,64)\n",
        "    x2=x\n",
        "    x = tf.layers.batch_normalization(x)\n",
        "    x = tf.nn.relu(x)\n",
        "\n",
        "    x = tf.nn.max_pool(x,\n",
        "                          ksize=[1, 3, 3, 1],\n",
        "                          strides=[1, 2, 1, 1],\n",
        "                          padding='SAME')\n",
        "    x = small_basic_block(x, 64,256)\n",
        "    x = tf.layers.batch_normalization(x)\n",
        "    x = tf.nn.relu(x)\n",
        "    x = small_basic_block(x, 256, 256)\n",
        "    x3 = x\n",
        "    x = tf.layers.batch_normalization(x)\n",
        "\n",
        "    x = tf.nn.relu(x)\n",
        "    x = tf.nn.max_pool(x,\n",
        "                       ksize=[1, 3, 3, 1],\n",
        "                       strides=[1, 2, 1, 1],\n",
        "                       padding='SAME')\n",
        "    x = tf.layers.dropout(x)\n",
        "\n",
        "    x = conv(x, 256, 256, ksize=[4, 1])\n",
        "    x = tf.layers.dropout(x)\n",
        "    x = tf.layers.batch_normalization(x)\n",
        "    x = tf.nn.relu(x)\n",
        "\n",
        "\n",
        "    x = conv(x,256,NUM_CHARS+1,ksize=[1,13],pad='SAME')\n",
        "    x = tf.nn.relu(x)\n",
        "    cx = tf.reduce_mean(tf.square(x))\n",
        "    x = tf.div(x,cx)\n",
        "\n",
        "    #x = tf.reduce_mean(x,axis = 2)\n",
        "    #x1 = conv(inputs,num_channels,num_channels,ksize = (5,1))\n",
        "\n",
        "\n",
        "    x1 = tf.nn.avg_pool(inputs,\n",
        "                       ksize=[1, 4, 1, 1],\n",
        "                       strides=[1, 4, 1, 1],\n",
        "                       padding='SAME')\n",
        "    cx1 = tf.reduce_mean(tf.square(x1))\n",
        "    x1 = tf.div(x1, cx1)\n",
        "\n",
        "    # x1 = tf.image.resize_images(x1, size = [18, 16], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "\n",
        "    x2 = tf.nn.avg_pool(x2,\n",
        "                        ksize=[1, 4, 1, 1],\n",
        "                        strides=[1, 4, 1, 1],\n",
        "                        padding='SAME')\n",
        "    cx2 = tf.reduce_mean(tf.square(x2))\n",
        "    x2 = tf.div(x2, cx2)\n",
        "\n",
        "    #x2 = tf.image.resize_images(x2, size=[18, 16], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "\n",
        "    x3 = tf.nn.avg_pool(x3,\n",
        "                        ksize=[1, 2, 1, 1],\n",
        "                        strides=[1, 2, 1, 1],\n",
        "                        padding='SAME')\n",
        "    cx3 = tf.reduce_mean(tf.square(x3))\n",
        "    x3 = tf.div(x3, cx3)\n",
        "\n",
        "    #x3 = tf.image.resize_images(x3, size=[18, 16], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "\n",
        "\n",
        "    #x1 = tf.nn.relu(x1)\n",
        "\n",
        "    x = tf.concat([x,x1,x2,x3],3)\n",
        "    x = conv(x, x.get_shape().as_list()[3], NUM_CHARS + 1, ksize=(1, 1))\n",
        "    logits = tf.reduce_mean(x,axis=2)\n",
        "    # x_shape = x.get_shape().as_list()\n",
        "    # outputs = tf.reshape(x, [-1,x_shape[2]*x_shape[3]])\n",
        "    # W1 = tf.Variable(tf.truncated_normal([x_shape[2]*x_shape[3],\n",
        "    #                                      150],\n",
        "    #                                     stddev=0.1))\n",
        "    # b1 = tf.Variable(tf.constant(0., shape=[150]))\n",
        "    # # [batch_size*max_timesteps,num_classes]\n",
        "    # x = tf.matmul(outputs, W1) + b1\n",
        "    # x= tf.layers.dropout(x)\n",
        "    # x = tf.nn.relu(x)\n",
        "    # W2 = tf.Variable(tf.truncated_normal([150,\n",
        "    #                                      NUM_CHARS+1],\n",
        "    #                                     stddev=0.1))\n",
        "    # b2 = tf.Variable(tf.constant(0., shape=[NUM_CHARS+1]))\n",
        "    # x = tf.matmul(x, W2) + b2\n",
        "    # x = tf.layers.dropout(x)\n",
        "    # # [batch_size,max_timesteps,num_classes]\n",
        "    # logits = tf.reshape(x, [b, -1, NUM_CHARS+1])\n",
        "\n",
        "    return logits, inputs, targets, seq_len\n",
        "\n",
        "def train(a):\n",
        "\n",
        "    train_gen = TextImageGenerator(img_dir=ti,\n",
        "                                   label_file=tl,\n",
        "                                   batch_size=BATCH_SIZE,\n",
        "                                   img_size=img_size,\n",
        "                                   num_channels=num_channels,\n",
        "                                   label_len=label_len)\n",
        "\n",
        "    val_gen = TextImageGenerator(img_dir=vi,\n",
        "                                 label_file=vl,\n",
        "                                 batch_size=BATCH_SIZE,\n",
        "                                 img_size=img_size,\n",
        "                                 num_channels=num_channels,\n",
        "                                 label_len=label_len)\n",
        "    global_step = tf.Variable(0, trainable=False)\n",
        "    learning_rate = tf.train.exponential_decay(INITIAL_LEARNING_RATE,\n",
        "                                               global_step,\n",
        "                                               DECAY_STEPS,\n",
        "                                               LEARNING_RATE_DECAY_FACTOR,\n",
        "                                               staircase=True)\n",
        "    logits, inputs, targets, seq_len = get_train_model(num_channels, label_len,BATCH_SIZE, img_size)\n",
        "    logits = tf.transpose(logits, (1, 0, 2))\n",
        "    # tragets\n",
        "    loss = tf.nn.ctc_loss(labels=targets, inputs=logits, sequence_length=seq_len)\n",
        "    cost = tf.reduce_mean(loss)\n",
        "\n",
        "    # optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=MOMENTUM).minimize(cost, global_step=global_step)\n",
        "    #optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss, global_step=global_step)\n",
        "    optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(loss, global_step=global_step)\n",
        "\n",
        "\n",
        "    #ctc_ greedy_decoder\n",
        "    decoded, log_prob = tf.nn.ctc_beam_search_decoder(logits, seq_len, merge_repeated=False)\n",
        "\n",
        "    acc = tf.reduce_mean(tf.edit_distance(tf.cast(decoded[0], tf.int32), targets))\n",
        "\n",
        "    init = tf.global_variables_initializer()\n",
        "\n",
        "    def report_accuracy(decoded_list, test_targets):\n",
        "        original_list = decode_sparse_tensor(test_targets)\n",
        "        detected_list = decode_sparse_tensor(decoded_list)\n",
        "        true_numer = 0\n",
        "\n",
        "        if len(original_list) != len(detected_list):\n",
        "            print(\"len(original_list)\", len(original_list), \"len(detected_list)\", len(detected_list),\n",
        "                  \" test and detect length desn't match\")\n",
        "            return\n",
        "        print(\"T/F: original(length) <-------> detectcted(length)\")\n",
        "        for idx, number in enumerate(original_list):\n",
        "            detect_number = detected_list[idx]\n",
        "            hit = (number == detect_number)\n",
        "            print(hit, number, \"(\", len(number), \") <-------> \", detect_number, \"(\", len(detect_number), \")\")\n",
        "            if hit:\n",
        "                true_numer = true_numer + 1\n",
        "        print(\"Test Accuracy:\", true_numer * 1.0 / len(original_list))\n",
        "\n",
        "    def do_report(val_gen,num):\n",
        "        for i in range(num):\n",
        "            test_inputs, test_targets, test_seq_len = val_gen.next_batch()\n",
        "            test_feed = {inputs: test_inputs,\n",
        "                        targets: test_targets,\n",
        "                        seq_len: test_seq_len}\n",
        "            st =time.time()\n",
        "            dd= session.run(decoded[0], test_feed)\n",
        "            tim = time.time() -st\n",
        "            print('time:%s'%tim)\n",
        "            report_accuracy(dd, test_targets)\n",
        "\n",
        "    def test_report(testi,files):\n",
        "        true_numer = 0\n",
        "        num = files//BATCH_SIZE\n",
        "\n",
        "        for i in range(num):\n",
        "            test_inputs, test_targets, test_seq_len = val_gen.next_batch()\n",
        "            test_feed = {inputs: test_inputs,\n",
        "                        targets: test_targets,\n",
        "                        seq_len: test_seq_len}\n",
        "            dd = session.run([decoded[0]], test_feed)\n",
        "            original_list = decode_sparse_tensor(test_targets)\n",
        "            detected_list = decode_sparse_tensor(dd)\n",
        "            for idx, number in enumerate(original_list):\n",
        "                detect_number = detected_list[idx]\n",
        "                hit = (number == detect_number)\n",
        "                if hit:\n",
        "                    true_numer = true_numer + 1\n",
        "        print(\"Test Accuracy:\", true_numer * 1.0 / files)\n",
        "\n",
        "\n",
        "    def do_batch(train_gen,val_gen):\n",
        "        train_inputs, train_targets, train_seq_len = train_gen.next_batch()\n",
        "\n",
        "        feed = {inputs: train_inputs, targets: train_targets, seq_len: train_seq_len}\n",
        "\n",
        "        b_loss, b_targets, b_logits, b_seq_len, b_cost, steps, _ = session.run(\n",
        "            [loss, targets, logits, seq_len, cost, global_step, optimizer], feed)\n",
        "\n",
        "        #print(b_cost, steps)\n",
        "        if steps > 0 and steps % REPORT_STEPS == 0:\n",
        "            do_report(val_gen,test_num)\n",
        "            saver.save(session, \"./model/LPRtf3.ckpt\", global_step=steps)\n",
        "        return b_cost, steps\n",
        "\n",
        "    with tf.Session() as session:\n",
        "        session.run(init)\n",
        "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=100)\n",
        "        if a=='train':\n",
        "            for curr_epoch in range(num_epochs):\n",
        "                print(\"Epoch.......\", curr_epoch)\n",
        "                train_cost = train_ler = 0\n",
        "                for batch in range(BATCHES):\n",
        "                    start = time.time()\n",
        "                    c, steps = do_batch(train_gen,val_gen)\n",
        "                    train_cost += c * BATCH_SIZE\n",
        "                    seconds = time.time() - start\n",
        "                    #print(\"Step:\", steps, \", batch seconds:\", seconds)\n",
        "\n",
        "                train_cost /= TRAIN_SIZE\n",
        "                val_cs=0\n",
        "                val_ls =0\n",
        "                for i in range(test_num):\n",
        "                    train_inputs, train_targets, train_seq_len = val_gen.next_batch()\n",
        "                    val_feed = {inputs: train_inputs,\n",
        "                                targets: train_targets,\n",
        "                                seq_len: train_seq_len}\n",
        "\n",
        "                    val_cost, val_ler, lr, steps = session.run([cost, acc, learning_rate, global_step], feed_dict=val_feed)\n",
        "                    val_cs+=val_cost\n",
        "                    val_ls+=val_ler\n",
        "\n",
        "                log = \"Epoch {}/{}, steps = {}, train_cost = {:.3f}, train_ler = {:.3f}, val_cost = {:.3f}, val_ler = {:.3f}, time = {:.3f}s, learning_rate = {}\"\n",
        "                print(log.format(curr_epoch + 1, num_epochs, steps, train_cost, train_ler, val_cs/test_num, val_ls/test_num,\n",
        "                                 time.time() - start, lr))\n",
        "        if a =='test':\n",
        "            testi='valid'\n",
        "            saver.restore(session, './model8.24best/LPRtf3.ckpt-25000')\n",
        "            test_gen = TextImageGenerator(img_dir=testi,\n",
        "                                           label_file=None,\n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           img_size=img_size,\n",
        "                                           num_channels=num_channels,\n",
        "                                           label_len=label_len)\n",
        "            do_report(test_gen,3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhJ1qA7DctPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = input('train or test:')\n",
        "train(a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhHCK4R1ntem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip synthetic-turkish-license-plates.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVuUfvWxI5tb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqukZ6f9mGKv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imports \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from keras.layers import Dense, Flatten, Input, Conv2D, MaxPooling2D\n",
        "from keras.models import Model, load_model\n",
        "from keras.utils import Sequence, plot_model\n",
        "# determining file path \n",
        "files = glob.glob(\"synthetic-turkish-license-plates/license-plates/*.png\")\n",
        "all_files = [(os.path.splitext(os.path.basename(file))[0], file) for file in files]\n",
        "#determining the images and the Labels and reomve the dash.\n",
        "X = [filename[1] for filename in all_files]\n",
        "y1 = np.array([plate_label[0].replace(\"-\", \"\")[0] for plate_label in all_files])\n",
        "y2 = np.array([plate_label[0].replace(\"-\", \"\")[1] for plate_label in all_files])\n",
        "y3 = np.array([plate_label[0].replace(\"-\", \"\")[2] for plate_label in all_files])\n",
        "y4 = np.array([plate_label[0].replace(\"-\", \"\")[3] for plate_label in all_files])\n",
        "y5 = np.array([plate_label[0].replace(\"-\", \"\")[4] for plate_label in all_files])\n",
        "y6 = np.array([plate_label[0].replace(\"-\", \"\")[5] for plate_label in all_files])\n",
        "y7 = np.array([plate_label[0].replace(\"-\", \"\")[6] for plate_label in all_files])\n",
        "# shuffle the data. \n",
        "X, y1, y2, y3, y4, y5, y6, y7 = shuffle(X, y1, y2, y3, y4, y5, y6, y7)\n",
        "categories = np.array(list(\"ABCDEFGHIJKLMNOPRSTUVYZ0123456789\"))\n",
        "onehot_enc = OneHotEncoder(sparse=False)\n",
        "onehot_enc.fit(categories.reshape(-1, 1))\n",
        "\n",
        "y1 = onehot_enc.transform(y1.reshape(-1, 1))\n",
        "y2 = onehot_enc.transform(y2.reshape(-1, 1))\n",
        "y3 = onehot_enc.transform(y3.reshape(-1, 1))\n",
        "y4 = onehot_enc.transform(y4.reshape(-1, 1))\n",
        "y5 = onehot_enc.transform(y5.reshape(-1, 1))\n",
        "y6 = onehot_enc.transform(y6.reshape(-1, 1))\n",
        "y7 = onehot_enc.transform(y7.reshape(-1, 1))\n",
        "\n",
        "X_train, X_test, \\\n",
        "y1_train, y1_test, \\\n",
        "y2_train, y2_test, \\\n",
        "y3_train, y3_test, \\\n",
        "y4_train, y4_test, \\\n",
        "y5_train, y5_test, \\\n",
        "y6_train, y6_test, \\\n",
        "y7_train, y7_test = \\\n",
        "    train_test_split(X, y1, y2, y3, y4, y5, y6, y7, test_size=0.2, random_state=42)\n",
        "\n",
        "class MyCustomGenerator(Sequence):\n",
        "    def __init__(self, image_filenames, labels, batch_size):\n",
        "        self.image_filenames = image_filenames\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "    \n",
        "    def __len__(self):\n",
        "        return (np.ceil(len(self.image_filenames) / float(self.batch_size))).astype(np.int)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = self.image_filenames[idx * self.batch_size : (idx+1) * self.batch_size]\n",
        "        batch_y1 = self.labels[0][idx * self.batch_size : (idx+1) * self.batch_size]\n",
        "        batch_y2 = self.labels[1][idx * self.batch_size : (idx+1) * self.batch_size]\n",
        "        batch_y3 = self.labels[2][idx * self.batch_size : (idx+1) * self.batch_size]\n",
        "        batch_y4 = self.labels[3][idx * self.batch_size : (idx+1) * self.batch_size]\n",
        "        batch_y5 = self.labels[4][idx * self.batch_size : (idx+1) * self.batch_size]\n",
        "        batch_y6 = self.labels[5][idx * self.batch_size : (idx+1) * self.batch_size]\n",
        "        batch_y7 = self.labels[6][idx * self.batch_size : (idx+1) * self.batch_size]\n",
        "        \n",
        "        return np.array([cv2.imread(filename) / 255.0\n",
        "                         for filename in batch_x]), \\\n",
        "            [np.array(batch_y1), np.array(batch_y2), np.array(batch_y3), np.array(batch_y4), np.array(batch_y5), np.array(batch_y6), np.array(batch_y7)]\n",
        "\n",
        "batch_size = 200\n",
        "\n",
        "training_batch_generator = MyCustomGenerator(X_train, (y1_train, y2_train, y3_train, y4_train, y5_train, y6_train, y7_train), batch_size)\n",
        "test_batch_generator = MyCustomGenerator(X_test, (y1_test, y2_test, y3_test, y4_test, y5_test, y6_test, y7_test), batch_size)\n",
        "\n",
        "WIDTH = 1025\n",
        "HEIGHT = 218\n",
        "CHANNEL = 3\n",
        "\n",
        "inputs = Input(shape=(HEIGHT, WIDTH, CHANNEL))\n",
        "conv1 = Conv2D(8, kernel_size=3, activation=\"relu\")(inputs)\n",
        "conv1 = Conv2D(8, kernel_size=3, activation=\"relu\")(conv1)\n",
        "conv1 = MaxPooling2D()(conv1)\n",
        "\n",
        "conv2 = Conv2D(16, kernel_size=3, activation=\"relu\")(conv1)\n",
        "conv2 = Conv2D(16, kernel_size=3, activation=\"relu\")(conv2)\n",
        "conv2 = MaxPooling2D()(conv2)\n",
        "\n",
        "conv3 = Conv2D(16, kernel_size=3, activation=\"relu\")(conv2)\n",
        "conv3 = Conv2D(16, kernel_size=3, activation=\"relu\")(conv3)\n",
        "conv3 = MaxPooling2D()(conv3)\n",
        "\n",
        "dense = Flatten()(conv3)\n",
        "\n",
        "dense = Dense(64, activation=\"relu\")(dense)\n",
        "\n",
        "dense_output1 = Dense(33, activation=\"softmax\")(dense)\n",
        "dense_output2 = Dense(33, activation=\"softmax\")(dense)\n",
        "dense_output3 = Dense(33, activation=\"softmax\")(dense)\n",
        "dense_output4 = Dense(33, activation=\"softmax\")(dense)\n",
        "dense_output5 = Dense(33, activation=\"softmax\")(dense)\n",
        "dense_output6 = Dense(33, activation=\"softmax\")(dense)\n",
        "dense_output7 = Dense(33, activation=\"softmax\")(dense)\n",
        "\n",
        "model = Model(inputs=inputs, \\\n",
        "              outputs=[dense_output1, dense_output2, dense_output3, dense_output4, dense_output5, dense_output6, dense_output7])\n",
        "model.summary()\n",
        "\n",
        "plot_model(model)\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit_generator(training_batch_generator, epochs=32)\n",
        "\n",
        "predictions = model.predict_generator(test_batch_generator)\n",
        "\n",
        "y1_hat = onehot_enc.inverse_transform(predictions[0])\n",
        "y2_hat = onehot_enc.inverse_transform(predictions[1])\n",
        "y3_hat = onehot_enc.inverse_transform(predictions[2])\n",
        "y4_hat = onehot_enc.inverse_transform(predictions[3])\n",
        "y5_hat = onehot_enc.inverse_transform(predictions[4])\n",
        "y6_hat = onehot_enc.inverse_transform(predictions[5])\n",
        "y7_hat = onehot_enc.inverse_transform(predictions[6])\n",
        "\n",
        "y1_test = onehot_enc.inverse_transform(y1_test)\n",
        "y2_test = onehot_enc.inverse_transform(y2_test)\n",
        "y3_test = onehot_enc.inverse_transform(y3_test)\n",
        "y4_test = onehot_enc.inverse_transform(y4_test)\n",
        "y5_test = onehot_enc.inverse_transform(y5_test)\n",
        "y6_test = onehot_enc.inverse_transform(y6_test)\n",
        "y7_test = onehot_enc.inverse_transform(y7_test)\n",
        "\n",
        "print(classification_report(y1_test, y1_hat))\n",
        "print(\"#\" * 40)\n",
        "print(classification_report(y2_test, y2_hat))\n",
        "print(\"#\" * 40)\n",
        "print(classification_report(y3_test, y3_hat))\n",
        "print(\"#\" * 40)\n",
        "print(classification_report(y4_test, y4_hat))\n",
        "print(\"#\" * 40)\n",
        "print(classification_report(y5_test, y5_hat))\n",
        "print(\"#\" * 40)\n",
        "print(classification_report(y6_test, y6_hat))\n",
        "print(\"#\" * 40)\n",
        "print(classification_report(y7_test, y7_hat))\n",
        "print(\"#\" * 40)\n",
        "\n",
        "test_image = next(iter(test_batch_generator))\n",
        "print(test_image[0][1].shape)\n",
        "plt.imshow(test_image[0][1])\n",
        "plt.show()\n",
        "\n",
        "prediction = model.predict(test_image[0][1].reshape(1, 218, 1025, 3))\n",
        "\n",
        "for ch in prediction:\n",
        "    print(onehot_enc.inverse_transform(ch)[0][0], end=\"\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BGNTeMw1v-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('BestModel.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UN0oAn8S2Gyr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_image = next(iter(test_batch_generator))\n",
        "print(test_image[0][10].shape)\n",
        "plt.imshow(test_image[0][10])\n",
        "plt.show()\n",
        "\n",
        "prediction = model.predict(test_image[0][10].reshape(1, 218, 1025, 3))\n",
        "\n",
        "for ch in prediction:\n",
        "    print(onehot_enc.inverse_transform(ch)[0][0], end=\"\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}